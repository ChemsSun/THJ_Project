{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14463938,"sourceType":"datasetVersion","datasetId":9238478}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport zipfile\n\n# dataset\nprint(\"Available in /kaggle/input/:\")\nfor item in os.listdir('/kaggle/input/skin-patches-dataset3/'):\n    print(item)\n\n# Find CSV\ndef find_csv_path(partial_name='data2_FSD_patches.csv'):\n    partial_lower = partial_name.lower()\n    for root, dirs, files in os.walk('/kaggle/input/skin-patches-dataset3/'):\n        for file in files:\n            if partial_lower in file.lower() and file.endswith('.csv'):\n                full_path = os.path.join(root, file)\n                print(f\"\\nFound CSV: {full_path}\")\n                return full_path\n    raise FileNotFoundError(\"CSV not found\")\n\ncsv_path = find_csv_path()\ndf = pd.read_csv(csv_path)\n\nprint(f\"\\nLoaded {df.shape[0]} rows\")\nprint(\"Columns:\", df.columns.tolist())\n\n\ndf['image_patch'] = df['image_patch'].str.replace('\\\\', '/')\ndataset_folder = os.path.basename(os.path.dirname(csv_path))\nbase_input = '/kaggle/input/skin-patches-dataset3/' + dataset_folder\nsample_path = df['image_patch'].iloc[0]\nif not sample_path.startswith('/kaggle'):\n    df['image_patch'] = base_input + '/' + df['image_patch'].str.lstrip('/')\n\n# Labels\nif 'label' in df.columns and df['label'].dtype == 'object':\n    df['label'] = df['label'].map({'no_skin': 0, 'non_skin': 0, 'skin': 1, 'Skin': 1, 'NoSkin': 0}).fillna(0).astype(int)\n\ny = df['label'].values\n\n# Image Loader \ndef load_images(paths):\n    images = []\n    missing = 0\n    for path in paths:\n        if os.path.exists(path):\n            img = load_img(path, target_size=(16,16))\n            img_raw = img_to_array(img)\n            images.append(img_raw / 255.0)\n        else:\n            missing += 1\n            images.append(np.zeros((16,16,3)))\n    print(f\"Loaded {len(images)} patches, {missing} missing\")\n    return np.array(images)\n\nX = load_images(df['image_patch'].values)\n\n# Model \nmodel = Sequential([\n    Input(shape=(16, 16, 3)),\n    Conv2D(32, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu', padding='same'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu', padding='same', name='last_conv'),\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation=None, name='logit')  # NO SIGMOID - raw logit output\n])\n\nmodel.compile(\n    optimizer=Adam(learning_rate=0.001),\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),  # from_logits=True\n    metrics=['accuracy']\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n\ntest_acc = model.evaluate(X_test, y_test)[1]\nprint(f\"\\nTest Accuracy: {test_acc:.4f}\")\n\n# Deep Learning Feature Computation\nlogits = model.predict(X).flatten()\npred_probs = tf.sigmoid(logits).numpy()\npredicted_labels = (pred_probs > 0.5).astype(int)\n\n# Feature 1: ann_logit (raw model output before sigmoid)\nann_logit = logits\n\n# Feature 2: ann_conf (absolute value of logit = model confidence)\nann_conf = np.abs(logits)\n\n# Feature 3: ann_energy (L2 norm of conv activation energy)\n# Create a temporary model to extract conv features\nconv_output_layer = model.get_layer('last_conv')\nconv_model = Model(inputs=model.layers[0].input, outputs=conv_output_layer.output)\nconv_maps = conv_model.predict(X)           # (N, H, W, 128)\ngap = conv_maps.mean(axis=(1, 2))           # Global Average Pooling -> (N, 128)\nann_energy = np.linalg.norm(gap, axis=1)    # L2 norm -> (N,)\n\npred_df = pd.read_csv(csv_path)  \npred_df['label_ann'] = predicted_labels\npred_df['ann_logit'] = ann_logit\npred_df['ann_conf'] = ann_conf\npred_df['ann_energy'] = ann_energy\n\npred_csv_path = '/kaggle/working/predictions_with_ann.csv'\npred_df.to_csv(pred_csv_path, index=False)\nprint(f\"\\nPredictions CSV saved with deep learning features to {pred_csv_path}\")\n\n# Reconstruction Loop\nrecon_dir = '/kaggle/working/reconstructions'\nos.makedirs(recon_dir, exist_ok=True)\n\nrecon_df = pd.read_csv(pred_csv_path)\nrecon_df['load_path'] = recon_df['image_patch'].str.replace('\\\\', '/')\nif not recon_df['load_path'].iloc[0].startswith('/kaggle'):\n    recon_df['load_path'] = base_input + '/' + recon_df['load_path'].str.lstrip('/')\n\nprint(\"\\nCreating overlaid segmentation visualizations...\")\n\nfor image_id in recon_df['image_id'].unique():\n    subset = recon_df[recon_df['image_id'] == image_id].reset_index(drop=True)\n    n_patches = len(subset)\n    if n_patches == 0: continue\n    \n    cols = int(np.ceil(np.sqrt(n_patches)))\n    rows = int(np.ceil(n_patches / cols))\n    patch_size = 16\n    canvas = np.zeros((rows * patch_size, cols * patch_size, 3), dtype=np.float32)\n    green_color = np.array([0, 255, 0], dtype=np.float32)\n    alpha = 0.4\n    \n    current_pos = 0\n    for r in range(rows):\n        for c in range(cols):\n            if current_pos >= n_patches: break\n            row_data = subset.iloc[current_pos]\n            patch_path = row_data['load_path']\n            label_ann = row_data['label_ann']\n            \n            if os.path.exists(patch_path):\n                patch_img = img_to_array(load_img(patch_path))\n            else:\n                patch_img = np.zeros((16, 16, 3))\n            \n            if patch_img.max() <= 1.0: patch_img *= 255\n            patch_img = patch_img.astype(np.float32)\n            y_start, x_start = r * patch_size, c * patch_size\n            canvas[y_start:y_start+patch_size, x_start:x_start+patch_size] = patch_img\n            \n            if label_ann == 1:\n                region = canvas[y_start:y_start+patch_size, x_start:x_start+patch_size]\n                blended = (alpha * green_color) + ((1 - alpha) * region)\n                canvas[y_start:y_start+patch_size, x_start:x_start+patch_size] = blended\n            current_pos += 1\n\n    canvas = np.clip(canvas, 0, 255).astype(np.uint8)\n    fig = plt.figure(figsize=(cols * 1.5, rows * 1.5))\n    plt.imshow(canvas)\n    plt.axis('off')\n    plt.tight_layout()\n    output_path = os.path.join(recon_dir, f'{image_id}_segmentation_overlay.png')\n    plt.savefig(output_path, dpi=200, bbox_inches='tight', pad_inches=0.1)\n    plt.close()\n\nzip_path = '/kaggle/working/reconstructions.zip'\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for root, dirs, files in os.walk(recon_dir):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), recon_dir))\n\nprint(f\"\\nReconstructions zipped to {zip_path}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:14:54.341109Z","iopub.execute_input":"2026-01-31T17:14:54.342163Z","iopub.status.idle":"2026-01-31T17:23:46.591304Z","shell.execute_reply.started":"2026-01-31T17:14:54.342127Z","shell.execute_reply":"2026-01-31T17:23:46.590434Z"}},"outputs":[{"name":"stdout","text":"Available in /kaggle/input/:\ndata2_FSD\n\nFound CSV: /kaggle/input/skin-patches-dataset3/data2_FSD/data2_FSD_patches.csv\n\nLoaded 28160 rows\nColumns: ['image_id', 'image_patch', 'mask_patch', 'label']\nLoaded 28160 patches, 0 missing\nEpoch 1/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 16ms/step - accuracy: 0.7731 - loss: 0.4588 - val_accuracy: 0.8260 - val_loss: 0.4508\nEpoch 2/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8371 - loss: 0.3568 - val_accuracy: 0.8784 - val_loss: 0.3037\nEpoch 3/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8830 - loss: 0.2734 - val_accuracy: 0.8642 - val_loss: 0.2866\nEpoch 4/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.8718 - loss: 0.2845 - val_accuracy: 0.8804 - val_loss: 0.2635\nEpoch 5/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.8864 - loss: 0.2611 - val_accuracy: 0.8837 - val_loss: 0.2487\nEpoch 6/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8948 - loss: 0.2484 - val_accuracy: 0.8715 - val_loss: 0.2790\nEpoch 7/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8990 - loss: 0.2315 - val_accuracy: 0.8753 - val_loss: 0.2742\nEpoch 8/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.8999 - loss: 0.2337 - val_accuracy: 0.8888 - val_loss: 0.2516\nEpoch 9/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9064 - loss: 0.2207 - val_accuracy: 0.8941 - val_loss: 0.2644\nEpoch 10/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9035 - loss: 0.2175 - val_accuracy: 0.8957 - val_loss: 0.2507\nEpoch 11/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9109 - loss: 0.2083 - val_accuracy: 0.8970 - val_loss: 0.2518\nEpoch 12/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9106 - loss: 0.2116 - val_accuracy: 0.8795 - val_loss: 0.2797\nEpoch 13/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9141 - loss: 0.1940 - val_accuracy: 0.8955 - val_loss: 0.2618\nEpoch 14/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 16ms/step - accuracy: 0.9149 - loss: 0.1900 - val_accuracy: 0.8759 - val_loss: 0.3322\nEpoch 15/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9187 - loss: 0.1873 - val_accuracy: 0.8673 - val_loss: 0.3141\nEpoch 16/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9234 - loss: 0.1773 - val_accuracy: 0.8853 - val_loss: 0.2759\nEpoch 17/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9270 - loss: 0.1714 - val_accuracy: 0.9035 - val_loss: 0.2765\nEpoch 18/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.9363 - loss: 0.1536 - val_accuracy: 0.8935 - val_loss: 0.3176\nEpoch 19/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.9336 - loss: 0.1568 - val_accuracy: 0.8935 - val_loss: 0.3221\nEpoch 20/20\n\u001b[1m564/564\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 15ms/step - accuracy: 0.9366 - loss: 0.1518 - val_accuracy: 0.8968 - val_loss: 0.3040\n\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8951 - loss: 0.3071\n\nTest Accuracy: 0.9000\n\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step\n\u001b[1m880/880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step\n\nPredictions CSV saved with deep learning features to /kaggle/working/predictions_with_ann.csv\n\nCreating overlaid segmentation visualizations...\n\nReconstructions zipped to /kaggle/working/reconstructions.zip\n","output_type":"stream"}],"execution_count":3}]}